{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tI9FTyXQ0tb4"
   },
   "source": [
    "The following illustration shows a generic base neural network model, Mp (Machine Learning model/ML-Model) trained on ImageNet dataset (Da). The base model is then used as the base for Transfer Learning, on Image Classification task (based on resnet18). The last layer of this pre-trained model (fully-connected/fc layer, A2) is then modified (to be A2') by a quantum means through a quantum machine learning framework: Pennylane.ai with a new dataset (Db), generating a new model (Mrt).\n",
    "\n",
    "The Quantum ML framework (which is open-source) provides convenient access to multiple quantum simulator and real quantum computer backend, including IBM real Quantum Computer on IBM Quantum Computing Experience (IBM Cloud) through an open-source Qiskit API (Application Programming Interface) accessible by Python programming language.\n",
    "\n",
    "It is recommended to run this project on Google Colab, utilizing the GPU accelerator. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZBrHSXJ4WsJ"
   },
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHaXgMqw4WsK"
   },
   "source": [
    "## Load required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2nN6nT04WsK",
    "outputId": "98923b17-1f22-449c-ddcf-bb198097eff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Building Simulator, this may take a while\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: pybind11 in /opt/homebrew/lib/python3.9/site-packages (2.9.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: pennylane-lightning in /opt/homebrew/lib/python3.9/site-packages (0.20.2)\n",
      "Requirement already satisfied: pennylane>=0.15 in /opt/homebrew/lib/python3.9/site-packages (from pennylane-lightning) (0.20.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.9/site-packages (from pennylane-lightning) (1.22.0)\n",
      "Requirement already satisfied: ninja in /opt/homebrew/lib/python3.9/site-packages (from pennylane-lightning) (1.10.2.3)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.9/site-packages (from pennylane>=0.15->pennylane-lightning) (2.6.3)\n",
      "Requirement already satisfied: semantic-version==2.6 in /opt/homebrew/lib/python3.9/site-packages (from pennylane>=0.15->pennylane-lightning) (2.6.0)\n",
      "Requirement already satisfied: cachetools in /opt/homebrew/lib/python3.9/site-packages (from pennylane>=0.15->pennylane-lightning) (4.2.4)\n",
      "Requirement already satisfied: autograd in /opt/homebrew/lib/python3.9/site-packages (from pennylane>=0.15->pennylane-lightning) (1.3)\n",
      "Requirement already satisfied: toml in /opt/homebrew/lib/python3.9/site-packages (from pennylane>=0.15->pennylane-lightning) (0.10.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.9/site-packages (from pennylane>=0.15->pennylane-lightning) (1.7.3)\n",
      "Requirement already satisfied: appdirs in /opt/homebrew/lib/python3.9/site-packages (from pennylane>=0.15->pennylane-lightning) (1.4.4)\n",
      "Requirement already satisfied: autoray in /opt/homebrew/lib/python3.9/site-packages (from pennylane>=0.15->pennylane-lightning) (0.2.5)\n",
      "Requirement already satisfied: future>=0.15.2 in /opt/homebrew/lib/python3.9/site-packages (from autograd->pennylane>=0.15->pennylane-lightning) (0.18.2)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q cirq\n",
    "!pip3 install -q torch\n",
    "!pip3 install -q qiskit\n",
    "!pip3 install -q pennylane\n",
    "!pip3 install -q matplotlib\n",
    "!pip3 install -q torchvision\n",
    "!pip3 install -q prettytable\n",
    "!pip3 install -q pennylane-cirq\n",
    "!pip3 install -q pennylane-qiskit\n",
    "!pip3 install pennylane-lightning\n",
    "        \n",
    "# System\n",
    "import time\n",
    "import os, platform, sys\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Pennylane\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane_qiskit import AerDevice\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Quantum computers/simulators\n",
    "import qiskit\n",
    "import cirq\n",
    "from qiskit import IBMQ\n",
    "from qiskit.providers.ibmq import least_busy\n",
    "from cirq import Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0C1oUApy4WsL",
    "outputId": "c99d97d8-c708-48e5-de01-3e07013ac257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Python version]: 3.9.10 (main, Jan 15 2022, 11:40:53) \n",
      "[Clang 13.0.0 (clang-1300.0.29.3)]\n",
      "[Deep Learning framework, Pytorch (Facebook) version]: 1.10.1\n",
      "[Qiskit (IBM) version]: 0.19.1\n",
      "[Quantum Machine Learning framework (Pennylane) version]: 0.20.0\n"
     ]
    }
   ],
   "source": [
    "print(\"[Python version]:\", sys.version)\n",
    "print(\"[Deep Learning framework, Pytorch (Facebook) version]:\", torch.__version__)\n",
    "print(\"[Qiskit (IBM) version]:\", qiskit.__version__)\n",
    "print(\"[Quantum Machine Learning framework (Pennylane) version]:\", qml.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiDsC_l74WsL"
   },
   "source": [
    "## Create & Save IBM Token Credential to local computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8lgGwTKz4WsL"
   },
   "outputs": [],
   "source": [
    "IBMQ.save_account('a63b6a1cf53003aefe2ae45e4c33c98cf203d4e30fe96cdb091d38a8f81145e22c207f3bacd65d90d9dd3e85022f0cace70d2aa471b555124450d14296f897b1', overwrite=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBzTqoUl4WsL"
   },
   "source": [
    "Hardware:\n",
    "- CPU Configuration: Laptop CPU, MBP M1 with 16gb ram  \n",
    "- GPU Configuration: GColab Server with NVidia V100 16GB GPU (usually)\n",
    "\n",
    "#### Dataset for face mask & no mask (about 1500 images for training and validation, in _data/faces directory)\n",
    "\n",
    "face mask/no_mask dataset - https://github.com/prajnasb/observations/tree/master/experiements/data\n",
    "\n",
    "- 2 categories, each category about 750 for images training & validation\n",
    "- Just for a quick go-through, set num_epochs = 1. ==> Training Accuracy will be about 93%\n",
    "- around 45 min per epoch on CPU, batch size=8\n",
    "\n",
    "If all things looks ok, num_epochs = 10 will produce quite a good result, but longer time to train on CPU \n",
    "- around 7 hours for 30 epochs on CPU, batch size=8\n",
    "- if high-end gpu is available, this should be just about 2 hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTHEjt9L4WsM"
   },
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "vJBTwD4Q4WsM",
    "outputId": "c9b14ad2-156d-41b6-d7a4-d59ea2db1eee"
   },
   "outputs": [],
   "source": [
    "dataset_dir = \"chest_xray\"\n",
    "\n",
    "num_epochs = 8                      # Number of training epochs (1 for testing, 8 for final)\n",
    "n_qubits = 4                        # Number of qubits \n",
    "step = 0.0004                       # Learning rate\n",
    "batch_size = 8                      # Number of samples for each training step\n",
    "q_depth = 6                         # Depth of the quantum circuit (number of variational layers)\n",
    "gamma_lr_scheduler = 0.1            # Learning rate reduction applied every 10 epochs.\n",
    "q_delta = 0.01                      # Initial spread of random quantum weights\n",
    "rng_seed = 3                        # Seed for random number generator\n",
    "start_time = time.time()            # Start of the computation timer\n",
    "\n",
    "model_fileext = \".pth\"\n",
    "log_fileext = \".log\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFUj64h14WsO"
   },
   "source": [
    "## Define Backend: Quantum Simulator or real Quantum Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azztf6L-4WsO"
   },
   "outputs": [],
   "source": [
    "def select_qc_backend():\n",
    "    valid_selections = ('1', '2', '3', '5', '9')\n",
    "    prompt = \"Please select source:\\n \\\n",
    "        Enter 1 if you don't know these are\\n \\\n",
    "        1: Lightning Pennylane.ai Quantum Simulator\\n \\\n",
    "        2: Qiskit Aer, IBM Quantum Simulator (local)\\n \\\n",
    "        3: Qiskit IBMQ:Aer, IBM Quantum Simulator (IBM Quantum Computing Experience)\\n \\\n",
    "        5: Google Cirq:Simulator (local)\\n \\\n",
    "        9: Qiskit IBMQ:Terra, IBM real Quantum Computer (IBM Quantum Computing Experience)\\n\"    \n",
    "    selection = input(prompt)\n",
    "    while not(selection in valid_selections):\n",
    "        selection = input(prompt)\n",
    "    return selection\n",
    "\n",
    "# select backend quantum computer\n",
    "qc_backend = int(select_qc_backend())\n",
    "if qc_backend == 1: # lightning pennylane\n",
    "    backend_name = \"simPennylane\"\n",
    "    print(\"=> Using Pennylane Quantum Computer Simulator (local)\")\n",
    "    dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "elif qc_backend == 2: # ibm quantum simulator (local) \n",
    "    backend_name = \"simIBMQLocal\"\n",
    "    print(\"=> Using IBM Quantum Computer Simulator (local)\")\n",
    "    dev = qml.device(\"qiskit.aer\", wires=n_qubits)\n",
    "elif qc_backend == 3: # ibm quantum simulator (cloud) \n",
    "    backend_name = \"simIBMQCloud\"\n",
    "    print(\"=> Using IBM Quantum Computer Simulator (IBM Quantum Computing Experience on IBM Cloud)\")\n",
    "    print(\"Loading IBMQ credentials...\")\n",
    "    IBMQ.load_account()\n",
    "    provider = IBMQ.get_provider('ibm-q')\n",
    "    backend = 'ibmq_qasm_simulator'\n",
    "    dev = qml.device(\"qiskit.ibmq\", wires=n_qubits, backend=backend)\n",
    "    #dev.capabilities()['backend']\n",
    "elif qc_backend == 5: # google quantum simulator (local) \n",
    "    backend_name = \"simGoogleLocal\"\n",
    "    print(\"=> Using Google Quantum Computer Simulator (local)\")\n",
    "    dev = qml.device(\"cirq.simulator\", wires=n_qubits)\n",
    "elif qc_backend == 9: # ibm real quantum computer (cloud)\n",
    "    backend_name = \"realIBMQCloud\"\n",
    "    print(\"=> Using real IBM Quantum Computer (IBM Quantum Computing Experience on IBM Cloud)\")\n",
    "    print(\"Loading IBMQ credentials...\")\n",
    "    IBMQ.load_account()\n",
    "    provider = IBMQ.get_provider('ibm-q')\n",
    "    print(\"Searching available least busy real IBM Quantum Computer...\")\n",
    "    backend = least_busy(provider.backends(filters=lambda x: x.configuration().n_qubits >= 4 \\\n",
    "        and not x.configuration().simulator \\\n",
    "        and x.status().operational==True))\n",
    "    print('using least busy backend:', backend)\n",
    "    dev = qml.device(\"qiskit.ibmq\", wires=n_qubits, backend=str(backend))\n",
    "    #dev.capabilities()['backend']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52J8LzUX4WsO"
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "now_str = now.strftime(\"%d%m%Y%H%M%S\")\n",
    "\n",
    "# model & log file name to generate\n",
    "# swgCQ_resnet18_ + selected backend name + max epochs + current date & time + extension\n",
    "#   - pytorch model file extension '.pth'\n",
    "#   - log file for the generated model file extension '.log'\n",
    "base_filename = \"swgCQ_\"\n",
    "interim_model_name = base_filename + backend_name \n",
    "hybrid_model_name = base_filename + backend_name + \"(\" + str(num_epochs) + \")-\" + now_str + model_fileext\n",
    "log_filename = base_filename + backend_name + \"(\" + str(num_epochs) + \")-\" + now_str + log_fileext\n",
    "\n",
    "train_val_filename = base_filename + backend_name + \"_train_val-results\" + now_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmxHgUH94WsO"
   },
   "outputs": [],
   "source": [
    "# USE NVidia CUDA (GPU) if available\n",
    "# ----------------------------------\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if is_cuda_available else \"cpu\")\n",
    "if is_cuda_available:\n",
    "    print (\"cuda is available, using:\", device)\n",
    "else:\n",
    "    print (\"cuda is not available, using:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1yvkeIy4WsO"
   },
   "source": [
    "## Prepare new dataset to retrain last layer (fc-layer) of pre-trained resnet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0aQLcyi4WsP"
   },
   "outputs": [],
   "source": [
    "# initialize data loaders\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            # Normalize input channels using mean values and standard deviations of ImageNet.\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "data_dir = dataset_dir # Images data, faces with and without mask\n",
    "image_datasets = {\n",
    "    x if x == \"train\" else \"validation\": datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in [\"train\", \"val\"]\n",
    "}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"validation\"]}\n",
    "class_names = image_datasets[\"train\"].classes\n",
    "\n",
    "# Initialize dataloader\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True)\n",
    "    for x in [\"train\", \"validation\"]\n",
    "}\n",
    "\n",
    "# function to plot images\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image from tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    # Inverse of the initial normalization operation.\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFJ218Xe4WsP"
   },
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders[\"validation\"]))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "# In order to get reproducible results, we set a manual seed for the\n",
    "# random number generator and re-initialize the dataloaders.\n",
    "\n",
    "torch.manual_seed(rng_seed)\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True)\n",
    "    for x in [\"train\", \"validation\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlf-n2lE4WsP"
   },
   "source": [
    "## Define quantum layer to replace the fc-layer of pre-trained resnet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4t6hriIw4WsP"
   },
   "source": [
    "### Variational Quantum Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfE7Kmxw4WsP"
   },
   "source": [
    "#### Define base quantum layers\n",
    "Define quantum layers that will compose the quantum circuit\n",
    "- Hadamard (for making superposition)\n",
    "- Ry & Entanglement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_SWZAKN4WsQ"
   },
   "outputs": [],
   "source": [
    "# 1st - Prepare the Quantum Gates\n",
    "def H_layer(nqubits):\n",
    "    \"\"\"Layer of single-qubit Hadamard gates.\n",
    "    \"\"\"\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "        \n",
    "def RY_layer(w):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "        \n",
    "def entangling_layer(nqubits):\n",
    "    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n",
    "    \"\"\"\n",
    "    # In other words it should apply something like :\n",
    "    # CNOT  CNOT  CNOT  CNOT...  CNOT\n",
    "    #   CNOT  CNOT  CNOT...  CNOT\n",
    "    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n",
    "        qml.CNOT(wires=[i, i + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuMjs-o44WsQ"
   },
   "source": [
    "#### Define Quantum Circuit\n",
    "Define the quantum circuit through the PennyLane qnode decorator. The structure is that of a typical variational quantum circuit:\n",
    "- Embedding layer: All qubits are first initialized in a balanced superposition of up and down states, then they are rotated according to the input parameters (local embedding).\n",
    "- Variational layers: A sequence of trainable rotation layers and constant entangling layers is applied.\n",
    "- Measurement layer: For each qubit, the local expectation value of the \n",
    "Z operator is measured. This produces a classical output vector, suitable for additional post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kw6nFNd4WsQ"
   },
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(q_input_features, q_weights_flat):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit.\n",
    "    \"\"\"\n",
    "    # Reshape weights\n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "\n",
    "    # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "    H_layer(n_qubits)\n",
    "    \n",
    "    # Embed features in the quantum node\n",
    "    RY_layer(q_input_features)\n",
    "    \n",
    "    # Sequence of trainable variational layers\n",
    "    for k in range(q_depth):\n",
    "        entangling_layer(n_qubits)\n",
    "        RY_layer(q_weights[k])\n",
    "        \n",
    "    # Expectation values in the Z basis\n",
    "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n",
    "    return tuple(exp_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dD_sKA0x4WsQ"
   },
   "source": [
    "#### Define a custom torch.nn.Module representing a dressed quantum circuit.\n",
    "\n",
    "This is a concatenation of:\n",
    "- A classical pre-processing layer (nn.Linear)\n",
    "- A classical activation function (torch.tanh)\n",
    "- A constant np.pi/2.0 scaling\n",
    "- The previously defined quantum circuit (quantum_net)\n",
    "- A classical post-processing layer (nn.Linear)\n",
    "\n",
    "The input of the module is a batch of vectors with 512 real parameters (features) and the output is a batch of vectors with two real outputs (associated with the two classes of images: face_mask and face_nomask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZZqCL3S4WsQ"
   },
   "outputs": [],
   "source": [
    "# 2nd - Prepare the Replacement Quantum Layer\n",
    "class DressedQuantumNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Torch module implementing the *dressed* quantum net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Definition of the *dressed* layout.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.pre_net = nn.Linear(512, n_qubits)\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n",
    "        self.post_net = nn.Linear(n_qubits, 2)\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        \"\"\"\n",
    "        Defining how tensors are supposed to move through the *dressed* quantum\n",
    "        net.\n",
    "        \"\"\"\n",
    "\n",
    "        # obtain the input features for the quantum circuit\n",
    "        # by reducing the feature dimension from 512 to 4\n",
    "        pre_out = self.pre_net(input_features)\n",
    "        q_in = torch.tanh(pre_out) * np.pi / 2.0\n",
    "\n",
    "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
    "        q_out = torch.Tensor(0, n_qubits)\n",
    "        q_out = q_out.to(device)\n",
    "        for elem in q_in:\n",
    "            q_out_elem = quantum_net(elem, self.q_params).float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "\n",
    "        # return the two-dimensional prediction from the postprocessing layer\n",
    "        return self.post_net(q_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoE_LpAD4WsR"
   },
   "source": [
    "#### Build a full hybrid classical-quantum network\n",
    "\n",
    "Following the transfer learning approach:\n",
    "\n",
    "- load the classical pre-trained network ResNet18 from the torchvision.models zoo\n",
    "- freeze all the weights since they should not be trained\n",
    "- replace the last fully connected layer with our trainable dressed quantum circuit (DressedQuantumNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEdjX9n94WsR"
   },
   "outputs": [],
   "source": [
    "# 3rd - Replace last layer of resnet-18 with defined quantum layer\n",
    "model_hybrid = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model_hybrid.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Notice that model_hybrid.fc is the last layer of ResNet18\n",
    "model_hybrid.fc = DressedQuantumNet()\n",
    "\n",
    "# Use CUDA or CPU according to the \"device\" object.\n",
    "model_hybrid = model_hybrid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIsbzqhE4WsR"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_hybrid = optim.Adam(model_hybrid.fc.parameters(), lr=step)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_hybrid, step_size=10, gamma=gamma_lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grjxEkZE4WsR"
   },
   "source": [
    "# 2. Retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKZ-q8wW4WsR"
   },
   "source": [
    "## Define how to retrain the DressedQuantumNet (last layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kC5MtT7G4WsR"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, temp_model_name):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 10000.0  # Large arbitrary number\n",
    "    best_acc_train = 0.0\n",
    "    best_loss_train = 10000.0  # Large arbitrary number\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"validation\"]:\n",
    "            if phase == \"train\":\n",
    "                # Set model to training mode\n",
    "                model.train()\n",
    "            else:\n",
    "                # Set model to evaluate mode\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            n_batches = dataset_sizes[phase] // batch_size\n",
    "            it = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                since_batch = time.time()\n",
    "                batch_size_ = len(inputs)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Track/compute gradient and make an optimization step only when training\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Print iteration results\n",
    "                running_loss += loss.item() * batch_size_\n",
    "                batch_corrects = torch.sum(preds == labels.data).item()\n",
    "                running_corrects += batch_corrects\n",
    "                message = \"     > Phase: {} Epoch: {}/{} Iter: {}/{} Batch time: {:.4f}\".format(\n",
    "                                    phase,\n",
    "                                    epoch + 1,\n",
    "                                    num_epochs,\n",
    "                                    it + 1,\n",
    "                                    n_batches + 1,\n",
    "                                    time.time() - since_batch,\n",
    "                                 )\n",
    "                # Print to screen with flush=True \n",
    "                print(message,\n",
    "                        end=\"\\r\",\n",
    "                        flush=True,\n",
    "                )\n",
    "                it += 1\n",
    "\n",
    "            # Print epoch results\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            message = \"     > Phase: {} Epoch: {}/{} Loss: {:.4f} Acc: {:.4f}\".format(\n",
    "                                \"train\" if phase == \"train\" else \"validation  \",\n",
    "                                epoch + 1,\n",
    "                                num_epochs,\n",
    "                                epoch_loss,\n",
    "                                epoch_acc,\n",
    "                            )\n",
    "\n",
    "            # Check if this is the best model wrt previous epochs\n",
    "            if phase == \"validation\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"validation\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "            if phase == \"train\" and epoch_acc > best_acc_train:\n",
    "                best_acc_train = epoch_acc\n",
    "            if phase == \"train\" and epoch_loss < best_loss_train:\n",
    "                best_loss_train = epoch_loss\n",
    "\n",
    "            train_Acc = \"{:.4f}\".format(best_acc_train)\n",
    "            train_Loss = \"{:.4f}\".format(best_loss_train)\n",
    "            val_Acc = \"{:.4f}\".format(best_acc)\n",
    "            val_Loss = \"{:.4f}\".format(best_loss)\n",
    "            \n",
    "            # Update learning rate\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "        \n",
    "        # save the retrained model at this epoch completion\n",
    "        # epoch is saved as epoch+1 (so to start at 1 instead of 0)\n",
    "        # ---------------------------------------------------------\n",
    "        model_at_epoch = temp_model_name + \"-at-epoch-\"+ str(epoch+1) \\\n",
    "            + \"(\" + str(num_epochs) + \")-\" + now_str + model_fileext\n",
    "        torch.save(model_hybrid.state_dict(), model_at_epoch)\n",
    "        \n",
    "        # save results of trained model at this epoch\n",
    "        # ------------------------------------------- \n",
    "        # append list of \"epoch, train accuracy, train loss, val accuracy, val loss\" per epoch completion\n",
    "        #   accumulate in train_val_results\n",
    "        if epoch==0:\n",
    "            # create first row\n",
    "            train_val_results = np.array([[epoch+1, best_acc_train, best_loss_train, best_acc, best_loss]])\n",
    "        else:\n",
    "            train_result_at_epoch = np.array([[epoch+1, best_acc_train, best_loss_train, best_acc, best_loss]])\n",
    "            # append new row\n",
    "            train_val_results = np.append(train_val_results, train_result_at_epoch, axis=0) \n",
    "        \n",
    "    # Write train_val_results to file\n",
    "    np.save(train_val_filename, train_val_results)\n",
    "    \n",
    "    # Print final results\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    time_elapsed = time.time() - since\n",
    "    total_training_time = \"{:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60)\n",
    "    total_training_time = \"{:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60)\n",
    "    return model, total_training_time, train_Acc, train_Loss, val_Acc, val_Loss, train_val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W515un9p4WsS"
   },
   "source": [
    "## Re-train the quantum layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSIFRUa64WsS"
   },
   "outputs": [],
   "source": [
    "model_hybrid, total_training_time, train_Acc, train_Loss, val_Acc, val_Loss, train_val_results = train_model(\n",
    "    model_hybrid, criterion, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs, temp_model_name = interim_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLWQ3uQ24WsS"
   },
   "outputs": [],
   "source": [
    "# SAVE final retrained Resnet-18 model\n",
    "# ------------------------------------\n",
    "torch.save(model_hybrid.state_dict(), hybrid_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzWx6VGW4WsS"
   },
   "outputs": [],
   "source": [
    "# PRINT training summary\n",
    "# ----------------------\n",
    "from prettytable import PrettyTable\n",
    "    \n",
    "t_summary = PrettyTable()\n",
    "t_summary.field_names = [\"QC backend\", \"epoch\", \"batch size\", \\\n",
    "    \"train time\", \"train Acc\", \"train Loss\", \"val Acc\", \"val Loss\"]\n",
    "t_summary.add_row([backend_name, num_epochs, batch_size, \\\n",
    "    total_training_time, train_Acc, train_Loss, val_Acc, val_Loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fM_nHTz4WsT"
   },
   "outputs": [],
   "source": [
    "for x in train_val_results:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwlIin2R4WsT"
   },
   "outputs": [],
   "source": [
    "epoch = []\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "val_acc =[]\n",
    "val_loss = []\n",
    "\n",
    "for i in range(0, len(train_val_results)):\n",
    "    epoch = np.append(epoch, train_val_results[i,0])\n",
    "    train_acc = np.append(train_acc, train_val_results[i,1])\n",
    "    train_loss = np.append(train_loss, train_val_results[i,2])    \n",
    "    val_acc = np.append(val_acc, train_val_results[i,3])\n",
    "    val_loss = np.append(val_loss, train_val_results[i,4])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZujqP2F4WsT"
   },
   "outputs": [],
   "source": [
    "# PLOT graph - training/validation accuracy, loss\n",
    "# -----------------------------------------------\n",
    "x1 = epoch\n",
    "x2 = epoch\n",
    "y1 = train_acc\n",
    "y2 = val_acc\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "plt.plot(x1, y1, 'o-')\n",
    "plt.title('Training Summary: ' + backend_name)\n",
    "plt.ylabel('train/val accuracy')\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "plt.plot(x2, y2, '.-')\n",
    "plt.xlabel('Epoch')\n",
    "# plt.ylabel('train/val accuracy')\n",
    "\n",
    "plt.legend([\"train accuracy\",\"val accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mD48Ev_4WsT"
   },
   "outputs": [],
   "source": [
    "# PLOT graph - training/validation accuracy, loss\n",
    "# -----------------------------------------------\n",
    "y1 = train_loss\n",
    "y2 = val_loss\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "plt.plot(x1, y1, 'o-')\n",
    "plt.title('Training Summary: ' + backend_name)\n",
    "plt.ylabel('train/val loss')\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "plt.plot(x2, y2, '.-')\n",
    "plt.xlabel('Epoch')\n",
    "# plt.ylabel('train/val loss')\n",
    "\n",
    "plt.legend([\"train loss\",\"val loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2_-GLx84WsT"
   },
   "outputs": [],
   "source": [
    "# INSPECT the retrained Resnet-18 model neural network architecture\n",
    "# -----------------------------------------------------------------\n",
    "model_arch = model_hybrid.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FVn8gz_4WsU"
   },
   "source": [
    "# 3. Test/Predict using the re-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IteZFe9w4WsU"
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=4, fig_name=\"Predictions\"):\n",
    "    images_so_far = 0\n",
    "    _fig = plt.figure(fig_name, figsize=(15,15))\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # inferencing\n",
    "        for _i, (inputs, labels) in enumerate(dataloaders[\"validation\"]):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Expectation value is probabilistic expected value of the result (measurement) of an experiment\n",
    "            # Pennylane implements the quantum measurement in such a way that:\n",
    "            #  - if the first qubit is non zero, it puts negative to the measurement\n",
    "            # expvals is expectation value of predicted class\n",
    "            expvals, preds = torch.max(outputs, 1)\n",
    "            # expvals_min is expectation value of another class as we only have 2 image classes/categories here\n",
    "            expvals_min, preds_min = torch.min(outputs, 1)\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images // 2, 2, images_so_far)\n",
    "                ax.axis(\"off\")\n",
    "                title = \"Detected as <\" + class_names[preds[j]] + \">, Expectation Value: \" + \\\n",
    "                    \"{:.5f}\".format(expvals[j]) + \" (\" + \\\n",
    "                    \"{:.5f}\".format(expvals_min[j]) + \")\" \n",
    "                ax.set_title(\"[{}]\".format(title))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "                if images_so_far == num_images:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhuWpvC84WsU"
   },
   "outputs": [],
   "source": [
    "visualize_model(model_hybrid, num_images=4, fig_name=\"Predictions\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "QMLImageClassification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
